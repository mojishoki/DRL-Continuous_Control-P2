{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# from ddpg_agent import Agent\n",
    "env = UnityEnvironment(file_name='Reacher_Windows_x86_64_20/Reacher.exe')\n",
    "#for 20 agents:\n",
    "# env = UnityEnvironment(file_name='Reacher_Windows_x86_64_20/Reacher.exe')\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-470403d1087a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mActor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;34m\"\"\"Actor (Policy) Model.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=1, fc1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first \n",
    "            hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.bn1(self.fc1(state)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=1, fcs1_units=400, fc2_units=300, atoms=51):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, atoms)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.bn1(self.fcs1(state)))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.softmax(self.fc3(x),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.5146,  0.5734, -0.2548,  1.0413])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.distributions.Normal(torch.tensor([0,0,0,0]).float(), torch.tensor([1,1,1,1]).float()).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 1e-5 \n",
    "if num_agents==1: # L2 weight decay\n",
    "    UPDATE_EVERY = 400\n",
    "    LEARN_NUM = 200   \n",
    "else:\n",
    "    UPDATE_EVERY = 20\n",
    "    LEARN_NUM = 10\n",
    "# number of learning passes\n",
    "# OU_SIGMA = 0.2          # Ornstein-Uhlenbeck noise parameter\n",
    "# OU_THETA = 0.15         # Ornstein-Uhlenbeck noise parameter\n",
    "\n",
    "EPSILON = 1.0           # explore->exploit noise process added to act step\n",
    "EPSILON_DECAY = 1e-6 \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, batch_size=BATCH_SIZE, buffer_size=BUFFER_SIZE, gamma= GAMMA,\n",
    "                 tau=TAU, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC, weight_decay=WEIGHT_DECAY,\n",
    "                 update_every=UPDATE_EVERY, learn_num=LEARN_NUM, eps=EPSILON, eps_decay=EPSILON_DECAY,rollout_length=5,\n",
    "                 atoms=51, V_max=7, V_min=0, soft_update=True, hard_update=350, random_seed=1, eps_gauss=False):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        #hyperparams\n",
    "        self.batch_size=batch_size\n",
    "        self.batch_size=batch_size\n",
    "        self.buffer_size=buffer_size\n",
    "        self.gamma=gamma\n",
    "        self.tau=tau\n",
    "        self.lr_actor=lr_actor\n",
    "        self.lr_critic=lr_critic\n",
    "        self.weight_decay=weight_decay\n",
    "        #the atoms\n",
    "        self.num_atoms=atoms\n",
    "        self.V_max=V_max\n",
    "        self.V_min=V_min\n",
    "        self.delta=(self.V_max-self.V_min)/(self.num_atoms-1)\n",
    "        self.atoms=torch.tensor([self.delta*i+self.V_min for i in range(self.num_atoms)])\n",
    "        \n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed, atoms=self.num_atoms).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed, atoms=self.num_atoms).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "         \n",
    "        # Replay memory\n",
    "        self.rollout_length=rollout_length\n",
    "        self.memory = ReplayBuffer(action_size, self.buffer_size, self.batch_size, random_seed, self.rollout_length)\n",
    "        \n",
    "        #update_every\n",
    "        self.update_every=update_every\n",
    "        self.learn_num=learn_num\n",
    "        self.soft=soft_update\n",
    "        self.t_step = 0\n",
    "        self.t_hard=0\n",
    "        self.hard= hard_update\n",
    "        \n",
    "        #gaussian noise\n",
    "        self.eps=eps\n",
    "        self.eps_decay=eps_decay\n",
    "        self.eps_gauss=eps_gauss\n",
    "        self.means=torch.tensor([0,0,0,0]).float()\n",
    "        self.stds=torch.tensor([1,1,1,1]).float()\n",
    "        self.gauss=torch.distributions.Normal(self.means, self.stds)\n",
    "        #counter\n",
    "        self.counter_step=0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                #either put a for loop here to update many times for a single time_step\n",
    "                for _ in range(self.learn_num):\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences, self.gamma)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.soft==False:\n",
    "            self.t_hard = (self.t_hard + 1) % self.hard\n",
    "        if add_noise:\n",
    "            if self.eps_gauss:\n",
    "                action += self.gauss.sample()*self.eps\n",
    "            else:\n",
    "                action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "    \n",
    "    def project_dist(self,dist,returns_tmp,gamma):\n",
    "        proj_atoms=torch.zeros((self.num_atoms,self.batch_size)).to(device)\n",
    "        dist=dist.permute([1,0]) #num_atoms*B\n",
    "#         print(\"dist.shape:\",dist.shape)\n",
    "#         print(\"dist\",dist)\n",
    "        for i in range(self.num_atoms):\n",
    "            #size B \n",
    "            to_proj=returns_tmp.squeeze()+gamma**self.rollout_length * self.atoms[i]\n",
    "#             print(\"to_proj.shape:\",to_proj.shape)\n",
    "            proj_atoms[i]=torch.clamp(torch.tensor(to_proj),self.V_min,self.V_max)\n",
    "#         print(\"proj_atoms:\",proj_atoms)    \n",
    "#         new_dist=torch.zeros_like(dist).to(device)\n",
    "        #immediate implementation of the eq (7) in Distributional paper\n",
    "#         for i in range(self.num_atoms):\n",
    "#             new_dist[i]=(torch.clamp(1-torch.abs(proj_atoms-self.atoms[i])/self.delta,0,1)*dist).sum(dim=0)\n",
    "#         print(\"previous new_dist:\",new_dist)\n",
    "        #implementation of algorithm 1 \n",
    "        #size 1* num_atoms * num_atoms*B = 1*B\n",
    "#         prev_new_dist=new_dist.clone()\n",
    "        new_dist=torch.zeros_like(dist).to(device)\n",
    "        #here below instead of dist[j] it uses actually this best_action_value below in the algorithm!\n",
    "#         best_action_value=self.atoms.to(device).matmul(dist)\n",
    "#         print(\"best_action_value.shape:\",best_action_value.shape)\n",
    "#         print(\"best_action_value:\",best_action_value)\n",
    "        for j in range(self.num_atoms):    \n",
    "            bj=torch.zeros(self.batch_size).to(device)\n",
    "            bj=(proj_atoms[j]-self.V_min)/self.delta\n",
    "            #shape B\n",
    "            u=torch.ceil(bj).long()\n",
    "            l=torch.floor(bj).long()\n",
    "            new_dist[l]+=dist[j]*(u.float()-bj)\n",
    "            new_dist[u]+=dist[j]*(bj-l.float())\n",
    "#         print(\"new new_dist.shape:\",new_dist.shape)\n",
    "#         print(\"new new_dist:\",new_dist)\n",
    "#         print(\"difference:\", prev_new_dist-new_dist)\n",
    "        return new_dist.permute([1,0])\n",
    "        \n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        with torch.no_grad():            \n",
    "            actions_next = self.actor_target(next_states[self.rollout_length-1])\n",
    "            Z_target_next = self.critic_target(next_states[self.rollout_length-1], actions_next)\n",
    "#             print(\"Z_target_next.shape:\",Z_target_next.shape)\n",
    "#             print(\"Z_target_next:\",Z_target_next)\n",
    "            # Compute returns_tmp size B*1\n",
    "#             print(\"rewards:\",rewards)\n",
    "            returns_tmp = np.array([rewards[i]*(gamma**i) for i in range(self.rollout_length)]).sum(axis=0)\n",
    "#             print(\"returns_tmp.shape:\",returns_tmp.shape)\n",
    "#             print(\"returns_tmp:\",returns_tmp)\n",
    "            #dimension B*self.num_atoms\n",
    "            Y_target = returns_tmp + (gamma**self.rollout_length * Z_target_next * (1 - dones[self.rollout_length-1]))\n",
    "#             print(\"Y_target.shape:\",Y_target.shape)\n",
    "#             print(\"Y_target:\",Y_target)\n",
    "            Y_target = self.project_dist(Y_target,returns_tmp,gamma)\n",
    "        \n",
    "#         print(\"Y_target.shape:\",Y_target.shape)\n",
    "#         print(\"Y_target:\",Y_target)\n",
    "#         Y_target=Y_target.detach()\n",
    "        # Compute critic loss\n",
    "        #shouldn't we just use the hubert loss again?\n",
    "        self.critic_local.train()\n",
    "        Z_expected = self.critic_local(states[0], actions[0])\n",
    "#         print(\"Z_expected.shape:\",Z_expected.shape)\n",
    "#         print(\"Z_expected:\",Z_expected)\n",
    "        #to perform prioritized here just add weight=BUFFER_SIZE/p with p the probability\n",
    "        critic_loss = -(Y_target*torch.log(Z_expected+1e-10)).sum(dim=1).mean()\n",
    "        \n",
    "#         print(\"critic_loss=:\",critic_loss)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states[0])\n",
    "        #basically it's about maximizing the Q(s,action_provided_by_actor) hence the negative sign, B*atoms matmul atoms*1\n",
    "#         self.critic_local.eval() # i am not sure about that ... \n",
    "        actor_loss = -self.critic_local(states[0], actions_pred).matmul(self.atoms.to(device)).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "#         print(\"actor_loss:\",actor_loss)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        \n",
    "        if self.soft_update:\n",
    "            self.soft_update(self.critic_local, self.critic_target, self.tau)\n",
    "            self.soft_update(self.actor_local, self.actor_target, self.tau)\n",
    "        elif self.t_hard==0:\n",
    "            self.hard_update(self.critic_local, self.critic_target)\n",
    "            self.hard_update(self.actor_local, self.actor_target)\n",
    "#         print(self.counter_step)\n",
    "        self.counter_step+=1\n",
    "        if self.eps_gauss:\n",
    "            self.eps=self.eps-self.eps_decay\n",
    "            \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        # we might want to implement a hard update instead of soft as done in the paper\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "    def hard_update(self,local_model,target_model):\n",
    "        target_model.load_state_dict(local_model.state_dict())\n",
    "\n",
    "            \n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.04, sigma=0.02):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample. here dt = 1\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.gauss(0,1) for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, rollout_length):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.rollout_length=rollout_length\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        starts = random.sample(list(range(len(self.memory)-self.rollout_length)), k=self.batch_size)\n",
    "        states,actions,rewards,next_states,dones=(list() for _ in range(5))\n",
    "        for i in range(self.rollout_length):\n",
    "            experiences= [self.memory[start+i] for start in starts]\n",
    "            states.append(torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device))\n",
    "            actions.append(torch.from_numpy(np.vstack([e.action for e in experiences  if e is not None])).float().to(device))\n",
    "            rewards.append(torch.from_numpy(np.vstack([e.reward for e in experiences  if e is not None])).float().to(device))\n",
    "            next_states.append(torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device))\n",
    "            dones.append(torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device))\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 128      # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4        # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 1e-5 \n",
    "if num_agents==1: # L2 weight decay\n",
    "    UPDATE_EVERY = 400\n",
    "    LEARN_NUM = 400   \n",
    "else:\n",
    "    UPDATE_EVERY = 20\n",
    "    LEARN_NUM = 10\n",
    "# number of learning passes\n",
    "# OU_SIGMA = 0.2          # Ornstein-Uhlenbeck noise parameter\n",
    "# OU_THETA = 0.15         # Ornstein-Uhlenbeck noise parameter\n",
    "\n",
    "EPSILON = 1.0           # explore->exploit noise process added to act step\n",
    "EPSILON_DECAY = 1e-6 \n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size,batch_size=128,\n",
    "    buffer_size=1000000,\n",
    "    gamma=0.99,\n",
    "    tau=0.001,\n",
    "    lr_actor=0.0001,\n",
    "    lr_critic=0.001,\n",
    "    weight_decay=1e-5,\n",
    "    update_every=20,\n",
    "    learn_num=10,\n",
    "    eps=1.0,\n",
    "    eps_decay=1e-06,\n",
    "    rollout_length=5,\n",
    "    atoms=51,\n",
    "    V_max=0.5,\n",
    "    V_min=0,\n",
    "    soft_update=False,\n",
    "    hard_update=350,\n",
    "    random_seed=1,\n",
    "    eps_gauss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shoki\\anaconda3\\envs\\ml-agents\\lib\\site-packages\\ipykernel_launcher.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 (945 sec)  -- \tMin: 0.0\tMax: 2.0\tMean: 0.7\tMov. Avg: 0.7\n",
      "Episode 2 (943 sec)  -- \tMin: 0.0\tMax: 1.5\tMean: 0.8\tMov. Avg: 0.7\n",
      "Episode 3 (956 sec)  -- \tMin: 0.2\tMax: 2.2\tMean: 0.8\tMov. Avg: 0.8\n",
      "Episode 4 (1015 sec)  -- \tMin: 0.0\tMax: 2.4\tMean: 0.8\tMov. Avg: 0.8\n",
      "Episode 5 (1020 sec)  -- \tMin: 0.3\tMax: 2.0\tMean: 0.9\tMov. Avg: 0.8\n",
      "Episode 6 (1027 sec)  -- \tMin: 0.0\tMax: 1.7\tMean: 0.8\tMov. Avg: 0.8\n",
      "Episode 7 (1040 sec)  -- \tMin: 0.3\tMax: 2.3\tMean: 1.0\tMov. Avg: 0.8\n",
      "Episode 8 (1050 sec)  -- \tMin: 0.2\tMax: 1.5\tMean: 0.7\tMov. Avg: 0.8\n",
      "Episode 9 (1062 sec)  -- \tMin: 0.3\tMax: 1.9\tMean: 0.8\tMov. Avg: 0.8\n",
      "Episode 10 (1072 sec)  -- \tMin: 0.4\tMax: 3.5\tMean: 1.2\tMov. Avg: 0.9\n",
      "Episode 11 (1082 sec)  -- \tMin: 0.2\tMax: 1.9\tMean: 0.9\tMov. Avg: 0.9\n",
      "Episode 12 (1089 sec)  -- \tMin: 0.1\tMax: 1.9\tMean: 0.8\tMov. Avg: 0.9\n",
      "Episode 13 (1101 sec)  -- \tMin: 0.0\tMax: 1.9\tMean: 0.7\tMov. Avg: 0.9\n",
      "Episode 14 (1114 sec)  -- \tMin: 0.0\tMax: 2.0\tMean: 0.8\tMov. Avg: 0.8\n",
      "Episode 15 (1125 sec)  -- \tMin: 0.1\tMax: 1.6\tMean: 0.9\tMov. Avg: 0.9\n",
      "Episode 16 (1136 sec)  -- \tMin: 0.2\tMax: 1.4\tMean: 0.7\tMov. Avg: 0.8\n",
      "Episode 17 (1148 sec)  -- \tMin: 0.1\tMax: 1.5\tMean: 0.6\tMov. Avg: 0.8\n",
      "Episode 18 (1160 sec)  -- \tMin: 0.1\tMax: 1.9\tMean: 0.8\tMov. Avg: 0.8\n",
      "Episode 19 (1172 sec)  -- \tMin: 0.1\tMax: 1.5\tMean: 0.7\tMov. Avg: 0.8\n",
      "Episode 20 (1182 sec)  -- \tMin: 0.2\tMax: 1.3\tMean: 0.7\tMov. Avg: 0.8\n",
      "Episode 21 (1195 sec)  -- \tMin: 0.2\tMax: 2.5\tMean: 0.9\tMov. Avg: 0.8\n",
      "Episode 22 (1206 sec)  -- \tMin: 0.4\tMax: 2.0\tMean: 0.9\tMov. Avg: 0.8\n",
      "Episode 23 (1218 sec)  -- \tMin: 0.0\tMax: 2.9\tMean: 0.8\tMov. Avg: 0.8\n",
      "Episode 24 (1228 sec)  -- \tMin: 0.3\tMax: 1.9\tMean: 0.9\tMov. Avg: 0.8\n",
      "Episode 25 (1240 sec)  -- \tMin: 0.4\tMax: 1.5\tMean: 0.8\tMov. Avg: 0.8\n",
      "Episode 26 (1251 sec)  -- \tMin: 0.2\tMax: 1.8\tMean: 0.9\tMov. Avg: 0.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d9054fbfc74c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmean_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmoving_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m \u001b[0mmean_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmoving_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-d9054fbfc74c>\u001b[0m in \u001b[0;36mddpg\u001b[1;34m(n_episodes, max_t, solved_score, consec_episodes, print_every, train_mode, actor_path, critic_path, eps, eps_decay)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;31m# save experience to replay buffer, perform learning step at defined interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mscores\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-363400b813c3>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                     \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-363400b813c3>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, experiences, gamma)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;31m#             print(\"Y_target.shape:\",Y_target.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;31m#             print(\"Y_target:\",Y_target)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             \u001b[0mY_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproject_dist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturns_tmp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;31m#         print(\"Y_target.shape:\",Y_target.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-363400b813c3>\u001b[0m in \u001b[0;36mproject_dist\u001b[1;34m(self, dist, returns_tmp, gamma)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_atoms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[1;31m#size B\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mto_proj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturns_tmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout_length\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matoms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;31m#             print(\"to_proj.shape:\",to_proj.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mproj_atoms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_proj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV_min\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "stats=f''\n",
    "mean_scores,min_scores,max_scores,moving_avgs=([] for _ in range(4))\n",
    "def ddpg(n_episodes=500, max_t=1000, solved_score=30.0, consec_episodes=100, print_every=1, train_mode=True,\n",
    "         actor_path='actor_ckpt.pth', critic_path='critic_ckpt.pth',eps=EPSILON,eps_decay=EPSILON_DECAY,eps_gauss=False):\n",
    "    \"\"\"Deep Deterministic Policy Gradient (DDPG)\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int)      : maximum number of training episodes\n",
    "        max_t (int)           : maximum number of timesteps per episode\n",
    "        train_mode (bool)     : if 'True' set environment to training mode\n",
    "        solved_score (float)  : min avg score over consecutive episodes\n",
    "        consec_episodes (int) : number of consecutive episodes used to calculate score\n",
    "        print_every (int)     : interval to display results\n",
    "        actor_path (str)      : directory to store actor network weights\n",
    "        critic_path (str)     : directory to store critic network weights\n",
    "\n",
    "    \"\"\"\n",
    "    global mean_scores,min_scores,max_scores,moving_avgs,stats # list of mean scores from each episode\n",
    "    stats+=f'{agent.batch_size}{agent.buffer_size}{agent.gamma}{agent.tau}{agent.lr_actor}{agent.lr_critic}{agent.weight_decay}'\n",
    "    best_score = -np.inf\n",
    "    scores_window = deque(maxlen=consec_episodes)  # mean scores from most recent episodes\n",
    "                                  # list of moving averages\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name] # reset environment\n",
    "        states = env_info.vector_observations                   # get current state for each agent      \n",
    "        scores = np.zeros(num_agents)                           # initialize score for each agent\n",
    "        agent.reset() #so the noise gets close to zero in some time_steps... but how many really?! oh my god ... \n",
    "        agent.eps_gauss=eps_gauss\n",
    "        start_time = time.time()\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states, add_noise=True)         # select an action\n",
    "            env_info = env.step(actions)[brain_name]            # send actions to environment\n",
    "            next_states = env_info.vector_observations\n",
    "            # get next state\n",
    "            rewards = env_info.rewards                          # get reward\n",
    "            dones = env_info.local_done                         # see if episode has finished\n",
    "            # save experience to replay buffer, perform learning step at defined interval\n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done)             \n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            if np.any(dones):                                   # exit loop when episode ends\n",
    "                break\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        min_scores.append(np.min(scores))             # save lowest score for a single agent\n",
    "        max_scores.append(np.max(scores))             # save highest score for a single agent        \n",
    "        mean_scores.append(np.mean(scores))           # save mean score for the episode\n",
    "        scores_window.append(mean_scores[-1])         # save mean score to window\n",
    "        moving_avgs.append(np.mean(scores_window))    # save moving average\n",
    "                \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {} ({} sec)  -- \\tMin: {:.1f}\\tMax: {:.1f}\\tMean: {:.1f}\\tMov. Avg: {:.1f}'.format(\\\n",
    "                  i_episode, round(duration), min_scores[-1], max_scores[-1], mean_scores[-1], moving_avgs[-1]))\n",
    "        \n",
    "        if train_mode and mean_scores[-1] > best_score:\n",
    "            torch.save(agent.actor_local.state_dict(), actor_path)\n",
    "            torch.save(agent.critic_local.state_dict(), critic_path)\n",
    "                  \n",
    "        if moving_avgs[-1] >= solved_score and i_episode >= consec_episodes:\n",
    "            print('\\nEnvironment SOLVED in {} episodes!\\tMoving Average ={:.1f} over last {} episodes'.format(\\\n",
    "                                    i_episode-consec_episodes, moving_avgs[-1], consec_episodes))            \n",
    "            if train_mode:\n",
    "                stats+=f'solved{i_episode}'\n",
    "                torch.save(agent.actor_local.state_dict(), stats+actor_path)\n",
    "                torch.save(agent.critic_local.state_dict(), stats+critic_path)\n",
    "                np.savetxt(stats+f'mean',mean_scores)\n",
    "                np.savetxt(stats+f'min',min_scores)\n",
    "                np.savetxt(stats+f'max',max_scores)\n",
    "                np.savetxt(stats+f'avg',moving_avgs)\n",
    "            break\n",
    "            \n",
    "    return mean_scores,min_scores,max_scores,moving_avgs,stats\n",
    "mean_scores,min_scores,max_scores,moving_avgs,stats = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8VGXWwPHfSSGBhFBCClV6rxKQoggCgr1hQewKqGtdy9rL+r6u67tr21VWVMTeWBEFLIgovYROKKEFEkglhDTSJs/7xzOEFkJCMjPJzPl+PvOZmTt35p7LhHvm6WKMQSmllO/y83QASimlPEsTgVJK+ThNBEop5eM0ESillI/TRKCUUj5OE4FSSvk4TQRKKeXjNBEopZSP00SglFI+LsDTAVRGs2bNTNu2bT0dhlJK1SmrV6/OMMZEnG6/OpEI2rZtS2xsrKfDUEqpOkVE9lRmP60aUkopH6eJQCmlfJzLE4GI+IvIWhGZ7XzeTkRWiMh2EflKROq5OgallFKn5o4SwYPAlmOe/x143RjTCTgI3OmGGJRSSp2CSxOBiLQCLgHedz4X4AJghnOXj4ArXRmDUkqpirm6RPAG8DhQ6nweDmQZY0qcz5OAluW9UUQmiUisiMSmp6e7OEyllPJdLksEInIpkGaMWX3s5nJ2LXeJNGPMVGNMjDEmJiLitN1glVJKnSFXjiMYClwuIhcDwUAYtoTQWEQCnKWCVsB+F8aglKqAo9RwIK+Q9Bx7O5BbREZuIcGB/ozsFkmrJg08HWKFEjPzOXS4mJ4tG3k6lDrNZYnAGPMk8CSAiAwHHjXGTBCRb4BxwJfArcAsV8WgVG23P+swu9LzOLdTs0rtn1NQzBcr9xIVFsw57cKJbhRMQbGDrSk5bNx3iKSD+aQeKiA9t5D2zUIZ3T2KQe3DcZQaYvdksnTnAXam5ZKaXUBKdgEZuUU4Sstft/z57+Po1bIRF/dqzvUDWtM0xHbwO5RfzBvz4/l2zT6MMQT4+9G4fiDPXdad4V0ia+zfxlFqSMspKEtOxkB4aD3CQ4PYnprDJ8v28Nu2NAR4Z0J/xvaMrrFj+xpxx+L1xySCS0WkPTYJNAXWAjcZYworen9MTIzRkcXK26zec5BJH8dyIK+IpU9cQIvG9SvcPyO3kNs+XMmmfdll26LCgsjMK6LYYf8fB/oLkQ2DaRZaj22pORQUlxIaFEBRSSlFjlIC/IT2ESFEhQUTHRZMVFgwkWFBRDYMolmovYWH1iMjt4if41L4aVMK6xKzCArw4+qzW9EhIoS3F+zg0OFiLundgvCQepSUlrJydybb03J5bEwX7jm/AyJCQkYei3dk0DW6If3aNMHfr7ya4fLlFpYw4f0VrE/MOuU+zUKDGD+wNYt3ZBC3L5tptw2odEKtrYwxpGYXEt0ouEY+T0RWG2NiTrufOxJBdWkiUN7mh/X7eeSb9YSH1CP5UAEvXt6DW4e0PW6f0lKDn/PimXQwn5s/WEnyocO8fePZRIUFs2J3JhuTsmjeuD59WjWiZ8tGtGhUv+w9BcUOFm/P4LdtaYQGBTCkQzgD2zWlQb2qVQTEp+bw4ZIEvl2TRGFJKYPaN+W5S3vQvUVY2T75RSU8PmMDszckc16nZmTmFRG3/2jCahpSj+GdI2gfEULTkCCaNAhEBApLSil2GM5p15TWTW01VImjlEmfrOaP+HQevbAL7SNCaBZaDxAy84o4kFtIWP1ARnWLol6AH4fyi7l+6jL2Zubz6V3ncHabJmfwjXiWMYZft6Tx+rx4Nidn8/XkwQxs17Tan6uJQKlaYktyNl/HJhK3P9v+Mi8pZXNyNgPaNuHdm2O49j9LiQoL5vOJg8re80d8OndOX0VocACRDYPIyC2ixFHKh7cPoP9Z1b9AnInMvCISM/Pp3aoRtif48YwxTF24i9fmxdOteRiX9m7O8C4RbEnOYf6WVBZuzyAzr6jczw4K8OOBkZ2YeF57Xp67helLE3jpyp7cPOisSsWWll3Ate8uIzO3iFeu6c0lvZtXuP+B3ELmbU5l9Z6DPDqmC1FhNfMLHGBHWi5Z+UXEtK3c95SYmc+fPl/DhqRDnBXegMy8IoZ1juDtG8+udiyaCJRyk/+ds5mlOw9wuNhBQZGD4EB/mjcOJjqsPtvTctiQdIh6/n70ad2I4EB/ggL86BAZyp9HdyYowJ9Xf9rKuwt3sfqZUTRuYOvhJ7y/nO2puYzpEU1aTgHFDsPjY7vQNTrsNNF43rElmRMVFDvIyi8uSwhBgX6UOAxvzo9n7sYUosKCSM0uZOJ57Xj6ku5VOu6+rMP86bM1rEvM4vqY1jx/efeTSj8ZuYU8/NU6luzI4EjTyAMjO/Hn0Z2rfqLlyMwrYswbC8nKL+LryYPpd5rSiTGGW6atZN3eLJ69rDtX9WvJ33/cyvSlCSx94gIiq5mgNBEo5QZbkrO56M1F9GnViNZNGxAc6M/hIgfJhw6TfKiApiH1GNe/FVf2bUmTkPJnU1mfmMUVby/hn9f24Zr+rdiRlsOo1xby2Jgu/GlERzefkef8tjWVF3/YTK+WjXjzhn5ValM4othRyuvz4pnyx046RoTy7b1DaBgcWPb6S7M3M31pAvec34GxPaP5nzmbycgtYt7Dw8ot5VSFMYZ7P1vDr1tSaRYahACzHzivrJE9r7CE9JxC2jYLKXvPL3EpTPpkNc9d2p07zm0HwO6MPEb843ceGd2Z+0d2qlZMlU0EdWIaaqVqq/cW7aJBPX8+vuMcGjUIPP0bytGrZSOiw4L5OS6Fa/q34pNle6jn78f1A1rXcLS12wVdoxjh7HV0phflQH8/Hh/blQHtmnL7h6uYunAXj1zYBbC/1j9fsZcr+rTg0TF22yW9mvPsrDjiU3PpEt2wWvHPXLuPHzel8JexXTm3YzOumbKUh75ax4e3DWDWun387cetZOYV8eo1vbmmfysKih28NGcznSJDuXnw0Sqwds1COK9TMz5fuZd7hncgwN/1MwHp7KNKnaGUQwX8sH4/18W0PuMkAODnJ1zYI4qF29PJyC3kv2v2cXGvaJqFBtVgtHWDiFT7lznAiC6RXNanBe8t2kVqdgEA05cmcLjYwT3DO5TtN6ZnNH4CczYmn/QZh4scrEvM4ts1SWWfcSr7sg7z/Kw4BrRtwqRh7enVqhEvXN6DhfHpDHt1AX/+ej0tGtdnQNsmPPLNej5cspv3F+0iMfMwz1/Wg8ATLvYTzjmL5EMF/LY1rdr/FpWhJQKlztD0pQk4Sg13Oov01XFh92g+XraHR75eT25hCbec0INIVd1jF3bhp03JvPZLPM9e1p3pS3ZzYfcoOkUd/eUf2TCYge2aMndjMg+P6oSIkF1QzF3TY4ndk1nWjtC6aX3+e88QIhueXGcft/8QD3+1jlJj+Oe1fcuqtMYPbM36xCzmb03j1XG9GXd2K4pLS3ngi7W8+MNmAvyEsT2iy+3yOqpbJNFhwXyyfA8X9nD9+AgtESivNn9LKhM/juVwkaPC/bYkZ1PiKK1wn2PlFpbw2Yo9XNSzeVm3x+o4p31TGtUP5I/4dHq2DKNf68bV/kxf1ya8AbcMbss3qxN54fs4sgtKuLecNpdLerdgR1ou8am5ALwwK47Vew9y7/CO/Oem/nx42wAycoq4Y/oqcgtLyt5X7CjlrfnbueLfSziYX8x/bu5Pm/CjfwsiwivX9GLlUyO5LqY1fn5CUIA/b994NjcMaE1IUABPX9Kt3NgD/P0YP7ANi7ZnkJCRV8P/MifTRKC8VlpOAY98s555m1OZtmT3KfebtW4fF725iIveXMS8zalUpgPF16sSySko4a7zql8aAFu3PbKrrR+/ZVDbGqkeUXDfiI6EBAUwY3USQzuG07ecBDu2x9HqoR/W7+fbtfu4/4KOPDqmC2N7RjOiayRvT+jHluQc7vl0NbEJmfztxy2Meu0PXpsXz8W9mvPLQ8M4r9PJc6KJyEk9qAL8/Xjlmt7EPjOqwh8R4we2plvzMDJyKxxvWyO015DySsYYJn2ymoXx6fRs2Yj4lBz+eHxEWQ+OI4pKShn52u8E+PkhwK6MPAa2bco/r+tT7n/S3MISvly5l38v2EGnyFC+uXtIjcW8LjGLKb/v4M0b+hEc6F9jn+vrpi7cyctzt/L5XecwpGP5I4/HT11O4sF8sg8X0z4ilBl3Dz6pkfarVXv5y383AhDgJwzuEM5Ng85ijBuqbs6U9hpSPu27dfuYtzmVpy/uxoiuEVz4+kL+9dt2nr+sx3H7fb5iD4mZh/nojoEM6RDOV6sSefWnrUx4fwXf3D24bKBRQbGDdxbs4MOlCeQUlDCofVP+ekXPGo25b+vGvHvzaf/Pqiq669z2DOscUeEYjIt7N+fZ7zbRoJ4/r1/ft9yeOtcPaENoUCAlpaUM7xJJo/pn3kGgttFEoLxOWnYBL3y/mf5nNeGOc9vh7ydcP6A1ny7fw+1D2pXV4+YUFPPWbzsY0iGcYZ2aISLcNOgserZsxIT3lnPT+yv4avJg0nMKefDLtWxNyeGintFMPr9DuVUMqnby85PTDsS7uGc0b83fzl/GdqXdMf38T3S6Ect1lSYC5XWm/LGTw0UOXh3Xu6wHx0OjOvPd2v288tMW3ri+H/UC/Hhv0W4y84r4y9iux9XJ923dmA9uG8Ct01ZyzZSl7M86TMPgAD68bQAjutbc7Jqq9ggPDWLV06M8HYbHaCJQXmfB1jSGdgynQ0Ro2baosGAmnteOt37bwa9bfqZHizC2JudwSa/m9Cnn1/2g9uH85+b+TP54NUM6hvN/4/oQ0dD3+vUr36CJQHmV3Rl5JBzI5/ahJ/fmeXBUZ7q3CGPN3izWJWYR0TCIx5wjTMszokska54bTUg9f+3Fo7yaJgLlVX7fZkdijihngRR/P2Fsz+aM7Vn5et7QIP0voryfjiNQXuX3bem0bxZy3MAepVTFXLl4fbCIrBSR9SISJyIvOrdPF5HdIrLOeevrqhiUbzlc5GDZrgM1ulyiUr7AleXeQuACY0yuiAQCi0XkR+drjxljZrjw2MpLbUvJwd8POkaePFPk8l0HKCopZXiXk0d4KqVOzWUlAmPlOp8GOm+1fxizqrVmb9jPZf9azPXvLi93pavft6VRP9C/Rpb4U8qXuLSNQET8RWQdkAbMM8ascL70vyKyQUReFxHtk6dO6/1Fu7jv87V0bd6QQ4eLeWn25uNeN8awYFs6QzqE6/QMSlWRSxOBMcZhjOkLtAIGikhP4EmgKzAAaAr8pbz3isgkEYkVkdj09HRXhqlqub//tJX/mbOFi3pG8/Xkwdw7oiMz1+5jwTFzte/OyGNvZr5WCyl1BtzSa8gYkwX8Dow1xiQ7q40KgQ+Bgad4z1RjTIwxJiYiQv9z+6ovV+5lyu87GT+wDf++8WyCA/3504gOdIoM5amZG8kpKGZ/1uGy2UW1oVipqnNZY7GIRADFxpgsEakPjAL+LiLNjTHJYkfoXAlsclUMqm5blZDJs7M2MaxzBC9d0aNsuoigAH9eHdeba6Ys5fz/+72sveD8zhE1sjaAUr7Glb2GmgMfiYg/tuTxtTFmtoj85kwSAqwD7nZhDKqO2pd1mLs/WU2rJg341w39TpoNsl+bJjw+titLdmQwrFME53eJoFNk6Ck+TSlVEV2PQNU6K3Yd4MmZG0nPLmTmn4bSUS/wSp0RXY9A1Tn7sw7z8twtzN6QTMvG9Zl6S4wmAaXcQBOBqhUSMvK44u0lFBQ7eGhUJyYP60D9etoNVCl30ESgPC6/qITJn6xGBH588DzaR2gpQCl30knnlEcZY/jLfzeyPS2Hf43vp0lAKQ/QRKA86oPFu/lh/X4eHdOF8zrpeBGlPEGrhpRLJGbmM3XhLu6/oCORzgXgjzDGsHB7Bu/+sZOlOw8wpkcU95zfwUORKqU0Eagal19UwsSPY9maksPWlGw+nziIQOc4gKSD+dz96Wo27csmKiyIpy7uyi2D2+oKYEp5kFYNqRpljOGxbzYQn5rDbUPasirhIH+buxWwpYTr313O3gP5vDquN4sev4BJwzroJHFKeZiWCFSNeuf3nczZmMyTF3Vl8vkdEIFpS3YTGRbEx0sTyCty8PnEQfRs2cjToSqlnLREoKpsW0oO//lj50nbV+/J5B+/bOOKvi2YNKw9AE9d3I2Ys5rwyo9bOVzs4POJ52gSUKqW0USgquy9Rbt45cetxKfmHLf9m9gkGgT687ere5XV+Qf6+/H2hLMZ178VX0waRI8WmgSUqm00EagqW77rAACzNySXbStxlPLL5lQu6BZFg3rH1zhGhQXzj2v70DU6zK1xKqUqRxOBqpLEzHySDh7GT2DOhv0cmbRwZUImmXlFXNwz2sMRKqWqShOBqpIVuzMBuPGcNuxMz2Obs3rop00pBAf6cb6uEKZUnaOJQFXJ8l0HaNIgkAdGdsJPYO6GZEpLDT9tSmF458iTqoWUUlVkDBw4AOvXw5w5kJHh8kPq/1pVJct3HeCcduFENgxmUPtwZm9MZljnCNJyCrmol1YLKVUphw/D3r2wcyfs2mXvj9x277avHzFnDlx8sUvD0USgKu1I+8Bd57YD4OJezXnmu0288et26vn7cUFXXS9YKQCKiuyFfvdue9u16+jjPXsgLe34/Rs0gPbtoVMnGDMG2rSBVq3srXt3l4fryjWLg4GFQJDzODOMMc+LSDvgS6ApsAa42RhT5Ko4VM050j4wqEM4AGN7RvPcrE0s3pHByK6RNAwO9GR4SrlXTs7Ri/yuXbBjB2zfbm+JiVBaenTfwEBo29be+vSBs86ytw4dbAKIigIPTrPiyhJBIXCBMSZXRAKBxSLyI/Bn4HVjzJci8h/gTmCKC+NQNeRI+0DnyIYANAsNYnCHcJbsOMBY7S2kvE1Jif31vn370V/zCQlHHx84cPz+jRvbX/Tnnmsv8O3aHb21bAn+tXcqFZclAmP7FeY6nwY6bwa4ALjRuf0j4AU0EdQJR9oH/PyO/nK5YUAbtqXkMLp7lAcjU+oMFRTYi3t5v+p377bJ4Ih69eyv+HbtoH9/e9+2rb3od+gATZp46iyqzaVtBCLiD6wGOgJvAzuBLGPMkX/dJKDlKd47CZgE0KZNG1eGqSrhSPvAxPPaH7f9sj4tuKxPCw9FpVQlFRTAtm0QFwebNtn7uDh78XeOhQEgJAQ6drTVN+PG2V/4HTva6pvmzcHPOztaujQRGGMcQF8RaQzMBLqVt9sp3jsVmAoQExNT7j7KfcraB9qHezgSpSqQnW0v8Bs3wubNEB9vb7t3H62z9/eHzp2hXz+YMMFe7Nu3t7/wo6M9WlfvKW7pNWSMyRKR34FBQGMRCXCWCloB+90RgzpzJY5SPlm+h4iGQXSK1KUkVS2RkgKxsbBmDaxbZ2+7dx99PSTEXvD794fx46FHD+jZ026rV89zcddCruw1FAEUO5NAfWAU8HdgATAO23PoVmCWq2JQNePdhbtYn5jFW+P7Hdc+oJTbZGbCqlX2wn/klpRkXxOxF/eBA+Guu6BXL3tr08Zrq3JqmitLBM2Bj5ztBH7A18aY2SKyGfhSRP4HWAt84MIYVDVtTcnmjV/jubhXNJf1bu7pcJQvKCiAtWthxQpYudLedh4z7XnnzjBsGAwYADExtoonJMRz8XoBV/Ya2gD0K2f7LmCgq46rak6xo5RHvl5Po/qBvHRFT11OUtU8h8PW5a9adfQX//r1UFxsX2/Vyv7SnzjRXvj794dGOpV5TdORxeqUpi7cRdz+bN69uT/hoUGeDkd5gwMHYPlyWLrU3q9cCbnOXuZhYfYX/p//DOecY28ttEeaO2giUOUqKHbw/qJdjOoWyZgeOlhMnQFjbH/8xYthyRJ727bNvubvD337wq23Hr3od+yodfoeoolAlev79fs5mF/MHc55hZQ6rdJS221z4UJ7W7QIUlPta+HhMGQI3HYbDB5sf/lrvX6toYlAncQYw/QlCXSJashgHTegTsUY22f/119hwQJ78c/Ksq+1bg2jR9tG3fPOgy5dfLJ/fl2hiUCdJHbPQTYnZx+39rBSAOzbZy/8v/wC8+cf/cXfoQNccw2cf7698Ldt69EwVdVoIlAnmb4kgUb1A7myb7mzfyhfkpsLf/wB8+bZ2+bNdntkJIwaZW8jR9o++6rO0kSgjrM/6zA/xaVw13ntqF+v9s6WqFzEGNi6FebOhR9/tPX8RUUQHGyreW6/3Vb59OqlDbteRBOBOs6ny/dgjOHmQWd5OhTlLkVFtn7/hx9g9mw7ERvYKRkeeADGjoWhQ20yUF5JE4E6zm9b0xjasRmtmjTwdCjKlQ4dsr/6Z82yv/yzs+2FfuRIeOwxuzSiVvf4DE0Eqkx+UQnxqTlcqGsLeKfkZPjuO3tbsMCO3o2Kguuug8svt0mggf4A8EWaCFSZuP3ZlBro3aqxp0NRNWXPHvjvf+1t2TLbBtCpEzz8MFx1lZ2+Qev6fZ4mAlVmfaLtA967tc7lUqclJMBXX8GMGXbuHrCjeP/6V7j6aujWTfv0q+NoIlBlNiQdonmjYCIbaqNgnZOcbC/+X35pZ+0EO0nbq6/a/v3t21f8fuXTNBGoMhuSsujdSksDdUZWFnz7LXz+Ofz2m6326dcP/v53W++vg7pUJWkiUAAcyi8m4UA+18a09nQoqiLFxbaXzyef2O6ehYV2srbnnrOrcHXp4ukIVR2kiUABsGGfbR/oow3FtdOGDTB9Onz6KaSnQ0QETJpk19wdOFDr/FW1uHKpytbAx0A0UApMNca8KSIvABOBdOeuTxlj5roqDlU5RxqKe2nVUO2Rnm6rfT76yK7YFRhou3neeqsd5BUY6OkIlZdwZYmgBHjEGLNGRBoCq0VknvO1140x/3DhsVUVrU86RLtmITSqrxcXj3I44Oef4YMP4PvvoaTErsr11ltw4412Omelapgrl6pMBpKdj3NEZAugs5jVUhuSshikU057Tlycrfb55BM7w2ezZnZ6h9tvh549PR2d8nJuaSMQkbbY9YtXAEOB+0TkFiAWW2o46I44VPlSswtIzS7UgWTulptrL/xTp8K6dXbVrjFj4M034bLLoF49T0eofITLhxSKSCjwX+AhY0w2MAXoAPTFlhj+eYr3TRKRWBGJTU9PL28XVUOOtA/00fYB99i1Cx55xC7Mfu+9dmTvm2/aksCcObbfvyYB5UYuLRGISCA2CXxmjPkWwBiTeszr7wGzy3uvMWYqMBUgJibGuDJOX1Raav9J/fyEDUmH8PcTerTQROAyxti+/m+9Zbt9+vvDtdfa6p9zztFeP8qjXNlrSIAPgC3GmNeO2d7c2X4AcBWwyVUxqPJtTcnmnk/XsDcznyYNAikoLqVTZKiuP+AKhYW2589rr8GmTbbb59NPwz33QIsWno5OKcC1JYKhwM3ARhFZ59z2FDBeRPoCBkgAJrswBnWCXzen8uCXawkJCmDysPYczC/mQG4hl/Ru7unQvMuBAzBlCvz733Y5x9694cMP4YYbdF5/Veu4stfQYqC88q6OGfCQaYt389KczfRs0Yj3bokhupFekGrc7t321/+0aZCfb/v7P/KIneJZq39ULaUji31EQbGDl+du4fzOEUyZ0F+rgWpaXBy88gp88YVt/J0wwSYA7fqp6gBNBD4ibn82JaWG8QPbaBKoSWvWwEsv2cVeQkLgoYfsXP8tdciMqjs0EfiIDUm2i2jf1jpWoEbExtr5/X/4ARo3tpO+PfCAjvxVdZImAh+xPjGLqLAgosK0XeCMHekC+sor8Ouv0KSJLQ3cfz800q63qu7SROAjNiQd0plFz1RpqZ335+WXYdUqiI62C75MngxhYZ6OTqlq08VKfcChw8Xsysijj1YLVU1JCXz2me36edVVkJkJ775rewY99pgmAeU1NBH4gI1JhwB09bHKKiqC99+3i7zcdJPt9vn557B1q10DQMcBKC9TqUQgItc6p5JGRJ4RkW9F5GzXhqZqynpnQ3HvlloiqFBJiU0AHTvCxInQtCnMnAnr19vVvwK0JlV5p8qWCJ51TiV9LjAG+Ag7eZyqA9YnZtm1BhroWgPlMgZmzLB9/idOhObN7XKQK1fClVfacQFKebHK/oU7nPeXAFOMMbMAnR6xjrANxVotVK7ly2HIEDsBnL+/LQEsX25HBOtIYOUjKpsI9onIu8B1wFwRCarCe5UHpWYXkJJdoGsNnCghwa74NXiwffz++3Zd4Cuv1ASgfE5lL+bXAT8DY40xWUBT4DGXRaVqTNlaA9pjyMrMhEcftQ3BM2fCM8/A9u1w5522RKCUD6pU65cxJl9E0oBzge3Y9Yi3uzIwVTPWJ2UR4Cf0aOHjXR2Li+Htt+HFF+HQIbjtNjsyuFUrT0emlMdVttfQ88BfgCedmwKBT10VlKo5G5IO0SW6IcGBPvxr96ef7FiAhx+GgQPtspDTpmkSUMqpslVDVwGXA3kAxpj9QENXBaVqRm5hCesSs3y3fSAlBcaNg4susl1Df/jhaFJQSpWpbCIoMsYY7GIyiEiI60JSNaGwxMHkT2LJL3JwRV8fWwnLGJg+Hbp1g9mz7dQQcXFw6aXaEKxUOSqbCL529hpqLCITgV+B91wXlqoOR6nh4a/WsWTHAV69pjeD2vvQjJi7d8OYMXD77XZcwPr18OSTuhi8UhWoVCIwxvwDmIFdiL4L8Jwx5l8VvUdEWovIAhHZIiJxIvKgc3tTEZknItud902qexLqeM9/v4m5G1N45pJuXNPfR+rBHQ54/XV78V+2zC4R+ccftneQUqpCp+01JCL+wM/GmFHAvCp8dgnwiDFmjXN6itUiMg+4DZhvjHlFRJ4AnsA2RKsasOdAHp8u38vtQ9ty13ntPR2OeyxbBvfdZxeJueQSu1Zw69aejkqpOuO0JQJjjAPIF5EqDU01xiQbY9Y4H+cAW4CWwBXYKSpw3l9ZpYhVhdY5xw1c298HLoQpKXDrrXZkcGoqfPmlbRDWJKBUlVR2Fq0CYKPzF33ekY3GmAcq82YRaQv0A1YAUcaYZOf7k0Uk8hTvmQRMAmjTpk0lw1Qbkw4RFOBHp6hQT4fiOg4HvPOOHQxWUGAS17lfAAAZuElEQVTbAJ56CkK9+JyVcqHKJoI5zluViUgotm3hIWNMtlSy14YxZiowFSAmJsacybF90YZ9h+jRIoxAfy+dASQ21k4FvXYtjB5tB4l16uTpqJSq0yo7svgjEakHdHZu2maMKT7d+0QkEJsEPjPGfOvcnCoizZ2lgeZA2pkErk7mKDVs2neI62K8sGqkqMguC/m3v0FkJHz1lZ0oTruDKlVtlR1ZPBw7pcTbwDtAvIgMO817BPgA2GKMee2Yl74HbnU+vhWYVcWYFZCQkcc5L//KtpScsm270nPJL3LQq6WXzTS6aRMMGgT/8z92oZgtW+C66zQJKFVDKlt/8E/gQmPM+caYYdg1CV4/zXuGAjcDF4jIOuftYuAVYLSIbAdGO5+rKlqx+wCp2YV8uzapbNsGb1uJrLQUXnsN+veHpCQ7Sdz06bpQvFI1rLJtBIHGmG1Hnhhj4p3VPqdkjFkMnOon28hKHledwraUXAB+iUvlibFdERE27jtEg3r+tI/wgkbTxETbI2jBArj8cnjvPVslpJSqcZUtEcSKyAciMtx5ew9Y7crAVMW2pWYDsDsjj+1pNimsT8qiZ8tG+PvV8SqTL7+08wGtXGkTwHffaRJQyoUqmwjuAeKAB4AHgc3A3a4KSp3etpRchneJQAR+2pRCsaOUzfuz6V2X2weysmwbwPjx0LWrnSX0rru0LUApF6ts1VAA8OaRRl/naOMgl0WlKnQgt5CM3ELO7diMnIISfo5LYVS3KApLSulVV9sHliyBCRNsW8CLL9pxAbpYvFJuUdkSwXyg/jHP62MnnlMesC3V9hTqEt2QMT2iiNufzY+bkgHq3pTTDoftFjpsmF0hbMkSeO45TQJKuVFlE0GwMSb3yBPn4wauCUmdzpEuozYRRAMwbfFuGgYH0Da8Dn0tSUlwwQX2wn/DDXaQ2DnneDoqpXxOZRNBnoicfeSJiMQAh10Tkjqd+NQcmjQIJCI0iLPCQ+ga3ZC8Ige9WzWisiO3Pe7776FPH1i92nYJ/fRTCPPx5TSV8pDKlr8fBL4Rkf3YxWlaANe7LCpVoa0pOXSJblh20R/TI5qtKTn0alkHqoUKC+Hxx+Gtt6BfP9tDqHPn079PKeUylS0RtMNOGncPdirqbThXK1PuZYwhPiWHLlFHVwq9tHdzAvyEwR1q+QI0CQlw3nk2CTz4oJ0+WpOAUh5X2RLBs8aYb0SkMXY08D+BKYBW6LpZ0sHD5BU56BJ9tBqlU1RDVj8zmkYNKhzj51mzZ8Mtt9jRwjNnwpU6+7hStUVlSwQO5/0lwH+MMbMAXfvPA+LLegwdP3q41iYBhwOefhouuwzatrVtApoElKpVKpsI9jnXLL4OmCsiQVV4r6pBR7qOdj6maqjWSk+HsWPt4vF33QVLl0KHDp6OSil1gspezK8DfgbGGmOygKbAYy6LSpXZmZ7LX2Zs4GBeEWC7jrZsXJ+GwbW0BHDEunV2srhFi+D99+1UEcHBno5KKVWOyq5HkA98e8zzZCDZVUGpo95buIuvYhPZnpbDZ3cNYpuzx1Ct9uOPdproxo3tALH+/T0dkVKqAlq9U4sVO0r5KS6FTpGhrE3M4v4v1rArPa92VwtNnWrbAzp2hBUrNAkoVQfoOP5abPGODLLyi/m/cX3Yn3WY57+PA05uKK4VjLFrCL/8Mlx0kV1BrGEtTlhKqTKaCGqx2euTaRgcwLDOzQgK8Cc1u4B3ft9Z++YTKi62jcEff2zvp0zRuYKUqkNcVjUkItNEJE1ENh2z7QUR2XfCimUKO1Asv6ik7HlhiYNfNqdwYfdoggL8AXhsTBdWPDWSDrVp4ZmcHLj0UpsE/vpXWzWkSUCpOsWVbQTTgbHlbH/dGNPXeZvrwuPXKS/P3cKgl+ezeb9dcGZRfAY5BSVc2qd52T4iQlRYLep5s2cPDB0K8+fDBx/As8/q2gFK1UEuSwTGmIVApqs+35ukZhfw0bI9ZBeUcPv0lezLOszsDftp3CCQczs283R45Vu2DAYOhL17bS+hO+7wdERKqTPkiV5D94nIBmfVURMPHL/WmbpwF45Sw/u3xJBf5ODWaSuZtzmVsT2iCfSvhR27Zs6EESNsY/Dy5TB6tKcjUkpVg7uvMlOADkBf7DiEf55qRxGZJCKxIhKbnp7urvjcLiO3kM9W7OHKvi0Z1T2KqTfHsPdAPnlFDi7t3cLT4Z1s5kw7RqBfP5sEunb1dERKqWpyayIwxqQaYxzGmFLgPWBgBftONcbEGGNiIiIi3Bekm72/aDeFJaXcO8JOvTC4Qzj/urEf15zdikHtm3o4uhPMmmWTQEwM/PwzNKul1VZKqSpxa/cOEWnuHJUMcBWwqaL9vdHXqxJZuD2d4V0i6demMZ8sS+DS3i2O6wk0pkd02cpjtcasWXDttXaA2E8/6SIySnkRlyUCEfkCGA40E5Ek4HlguIj0xa5lkABMdtXxayNjDG/8Gk9KdgGzNxydoeO+ER09GFUlfPop3HabTQI//wyNGnk6IqVUDXJZIjDGjC9n8weuOl5dELc/m/2HCnj1mt50bxHGr1tSaVQ/sHbPHfT223DffXZt4e++09HCSnkhHfnjRr9sTsVPYGS3SMJDg+jZspb/sn75ZbuWwOWX2ykjdPZQpbxSLeyb6L3mbU6l/1lNCA8N8nQoFTMGnnzSJoEJE2DGDE0CSnkxTQRukpiZz5bkbEZ3j/J0KBUrLYUHHoBXXoHJk+3UEYG1fO0DpVS1aCJwk1+3pAIwunst6w10rJISuPNO+Pe/4ZFH7ORxfvonopS30zYCN5m3OZWOkaG0axbi6VDKV1gI48fbAWMvvADPPafzBinlI/Tnnhscyi9mxe7M2lstlJtrZxCdORPeeAOef16TgFI+REsEbrBgWxqOUlM7E8G+fXDVVbB6NUyfDrfe6umIlFJuponADX7alEJEwyD61rYFZZYtg6uvtiWCmTNtN1GllM/RqiEX25qSzc+bU7i6X0v8/GpRdcuHH8Lw4RASYieP0ySglM/SROBi//fTNkKDArhneAdPh2IZYxuC77gDhg2DlSuhRw9PR6WU8iBNBC60KiGT+VvTuPv8DjRuUM/T4di1hW+/HV56ySaCuXOhaS2b4VQp5XaaCFzEGMPff9xKZMMg7hjaztPhQF4eXHYZfPSR7R76/vs6UEwpBWhjscvM35JG7J6D/O9VPalfz9+zwWRmwiWX2Gqg996Du+7ybDxKqVpFE4EL7D2Qzws/xNGuWQjXxbT2bDD798OYMRAfb+cMuuoqz8ajlKp1NBHUsC3J2dwybSXFjlKm3z7Qs2sOr18PV14JGRm2PWDkSM/FopSqtbSNoAatSsjkuneX4S/CN5MH07e1B8cNfPYZDB4MRUXw22+aBJRSp6SJoIYYY7j3szU0Cw1ixj2D6RTloQVciovhwQfhpptgwABYs8beK6XUKbgsEYjINBFJE5FNx2xrKiLzRGS7876Jq47vbrsz8kjPKeTu89vTqkkDzwSRkQEXXghvvQUPPQS//gpRtXBaC6VUreLKEsF0YOwJ254A5htjOgHznc+9wpq9WQD0a+Oh3LZ+PcTE2GkjPv4YXn9du4cqpSrFZYnAGLMQyDxh8xXAR87HHwFXuur47rZ270EaBgXQMSLUvQd2OOyMoUOG2PUEFi2Cm292bwxKqTrN3W0EUcaYZADnfeSpdhSRSSISKyKx6enpbgvwTK3dm0XfNo3dO5/Qhg22Qfjhh+H882HVKm0PUEpVWa1tLDbGTDXGxBhjYiIiIjwdToXyCkvYmpJNP3f2Epo2Dfr3h4QE+PxzmDMHmjd33/GVUl7D3YkgVUSaAzjv09x8fJfYkHSIUuOm9gFj4MUX7ZKSF1wAW7bYlcV0IRml1BlydyL4Hjiy8smtwCw3H98l1iYeBHD9uIGiIpg40c4VdNttMHs2hIe79phKKa/nyu6jXwDLgC4ikiQidwKvAKNFZDsw2vm8zluzJ4v2zUJoEuLCGUZjY239/wcfwLPP2qoh7RWklKoBLptiwhgz/hQvedUQV2MM6xIPMqyzi9oxCgrsGsL/+AdER8OsWbqIjFKqRulcQ9WUdPAwGblFnO2K9oGkJLuU5KpVtkro1VehcS1b7lIpVedpIqimNXtt+0C/NjV8gV60CMaNg8OH4bvv4IoravbzlVLKqdZ2H63Nlu86wJ4DeYAdP9Cgnj9dampuodJSeO012yOocWNYsUKTgFLKpbREUEXxqTncMHU5AGe3aUxqdiG9WzUioCamm96/3/YGmjfPXvw/+ggaNar+5yqlVAW0RFBF38QmEuAn/Hl0Z/IKHezLOsyg9jXQhfPnn6F3b1i8GN59F2bO1CSglHILLRFUQbGjlG/X7GNUtygeGNmJ+y/oSGLmYaIaBVXvg6dPt8tH9ugBX30FXbvWSLxKKVUZWiKogvlb0jiQV8R1A1oBICK0CW9AUMAZrklsDLz8Mtx+O4wYYRuINQkopdxME0EVfBObSGTDIIZ1qoExAyUlcN998PTTcOONdq6gsLDqf65SSlWRJoJKSs0uYMG2NK7p36r6DcN5eXZ8wDvvwGOPwSefQD0XjkpWSqkKaBtBJX27Zh+lBq7t36p6H5SaCpdeapeQfPttuPfemglQKaXOkCaCSjDG8E1sIgPaNqF9dRaeSU216wYkJtpBYpddVnNBKqXUGdKqoUrYkHSIXRl5jKtOaSAzE0aPtkng5581CSilag0tEVTCL5tT8PcTLuwefWYfkJ0NY8dCfLydOvrcc2s2QKWUqgZNBJXwc1wqA9s2PbNpprOy4JJLYO1aO0hs1KiaD1AppapBq4ZOY2d6LjvSchnTI6rqb05Ls+MDVq2yA8UuvbTmA1RKqWrSEsFpzNucCsDoHlWsFkpMtG0Ce/fCDz/AmDEuiE4pparPI4lARBKAHMABlBhjYjwRR2X8EpdCz5ZhtGxcv/Jv2r7dJoGDB+GXX7RNQClVq3myRDDCGJPhweOfVlp2AWv2ZvHI6M6Vf9PGjTYJOBywYAGcfbbrAlRKqRqgbQQVmLfFVgtdWNlqoZUr7TiBgABYuFCTgFKqTvBUIjDALyKyWkQmeSiG0/olLpW24Q3oHFWJQWSrVtkeQU2a2Kmku3VzfYBKKVUDPFU1NNQYs19EIoF5IrLVGLPw2B2cCWISQJs2bdweYGZeEUt3ZnD70HaISMU7b9hgG4ObNYM//oBW1ZyGQiml3MgjJQJjzH7nfRowExhYzj5TjTExxpiYiIgamO2zChIz87nu3WUAXNG3RcU7x8fbNoEGDWD+fE0CSqk6x+2JQERCRKThkcfAhcAmd8dxKusSs7jqnSWkZRfw8R3n0KNFBauELV4Mw4fbx/PnQ7t2bolRKaVqkidKBFHAYhFZD6wE5hhjfvJAHCdZu/cgN0xdRv16/nx771AGdzjFEpSlpXZBmeHDj5YEunRxa6xKKVVT3N5GYIzZBfRx93FPJ7+ohIe/Wkd4SBAz7x1Ks9BTLD+ZmwvjxtmJ466/HqZO1QVllFJ1mo4sdnp57hb2ZObzxcRBFSeBiy+GpUthyhSYPBlO15CslFK1nCYC4PdtaXy6fC8Tz2vHoPanqA7Ky7OTxy1ZAp9/bksDSinlBXx+QNnBvCIen7GBLlENeeTCU9Tz5+TYCeMWL4bPPtMkoJTyKj5dIjDG8PR3GzmYX8SHtw8gOND/5J3S02110Nq1dm3hG25wf6BKKeVCPp0Ivl2zj7kbU3h8bJfyu4kmJMCFF0JSkl1aUqeRVkp5IZ+tGkrMzOf57+MY2LYpk4d1OHmHX36BIUNsieDXXzUJKKW8lk8mAkep4ZGv1wPwz+v64O93TM+f9HS4+WY7ZUTDhrBokU0ISinlpXwyEXy4ZDcrEzJ58fIetG7a4OgL331nJ4v78kt45hlYvx569vRcoEop5QY+lwhSswt449ftDO8SwdVnt7QbCwrg/vvhqqvgrLNsw/BLL0FwsGeDVUopN/C5xuKX526hyFHKi5f3sLOKxsfb7qDr1sHDD8Pf/gZBpxhQppRSXsinEsGynQeYtW4/D4zsxFnhIXZg2OTJUK+eXVdYG4SVUj7IZ6qGih2lPDdrE62a1Ofec1rAxIkwYQL06WPbAjQJKKV8lM+UCKYu3MX2tFw+G96M4GHn2ov/k0/CX/9ql5ZUSikf5RNXwFUJmbw2L54nSrYz9IYJ4OcHc+bYEcNKKeXjvD4RZOYV8ehHy/nfJdO5YeE3dkH5GTN0ERmllHLy6kRQWmp45+VP+PA/L9A+MwnuuQdee027hSql1DG8OhHETnqUJ6e9weHIaDtNxMiRng5JKaVqHY/0GhKRsSKyTUR2iMgTrjqOX8cOrBx5NSHxWzQJKKXUKYgxxr0HFPEH4oHRQBKwChhvjNl8qvfExMSY2NhYN0WolFLeQURWG2NiTrefJ0oEA4Edxphdxpgi4EvgCg/EoZRSCs8kgpZA4jHPk5zbjiMik0QkVkRi09PT3RacUkr5Gk8kgvJWez+pfsoYM9UYE2OMiYmIiHBDWEop5Zs8kQiSgNbHPG8F7PdAHEoppfBMIlgFdBKRdiJSD7gB+N4DcSillMID4wiMMSUich/wM+APTDPGxLk7DqWUUpZHBpQZY+YCcz1xbKWUUsfzmWmolVJKlc/tA8rOhIikA3vO8O3NgIwaDKeu8MXz9sVzBt88b188Z6j6eZ9ljDltt8s6kQiqQ0RiKzOyztv44nn74jmDb563L54zuO68tWpIKaV8nCYCpZTycb6QCKZ6OgAP8cXz9sVzBt88b188Z3DReXt9G4FSSqmK+UKJQCmlVAW8OhG4awEcTxKR1iKyQES2iEiciDzo3N5UROaJyHbnfRNPx1rTRMRfRNaKyGzn83YissJ5zl85pzDxKiLSWERmiMhW53c+2Nu/axF52Pm3vUlEvhCRYG/8rkVkmoikicimY7aV+92K9Zbz2rZBRM6uzrG9NhE4F8B5G7gI6A6MF5Huno3KJUqAR4wx3YBBwJ+c5/kEMN8Y0wmY73zubR4Ethzz/O/A685zPgjc6ZGoXOtN4CdjTFegD/b8vfa7FpGWwANAjDGmJ3Zamhvwzu96OjD2hG2n+m4vAjo5b5OAKdU5sNcmAnxkARxjTLIxZo3zcQ72wtASe64fOXf7CLjSMxG6hoi0Ai4B3nc+F+ACYIZzF2885zBgGPABgDGmyBiThZd/19ipcOqLSADQAEjGC79rY8xCIPOEzaf6bq8APjbWcqCxiDQ/02N7cyKo1AI43kRE2gL9gBVAlDEmGWyyACI9F5lLvAE8DpQ6n4cDWcaYEudzb/y+2wPpwIfOKrH3RSQEL/6ujTH7gH8Ae7EJ4BCwGu//ro841Xdbo9c3b04ElVoAx1uISCjwX+AhY0y2p+NxJRG5FEgzxqw+dnM5u3rb9x0AnA1MMcb0A/Lwomqg8jjrxK8A2gEtgBBstciJvO27Pp0a/Xv35kTgMwvgiEggNgl8Zoz51rk59UhR0Xmf5qn4XGAocLmIJGCr/C7AlhAaO6sPwDu/7yQgyRizwvl8BjYxePN3PQrYbYxJN8YUA98CQ/D+7/qIU323NXp98+ZE4BML4Djrxj8AthhjXjvmpe+BW52PbwVmuTs2VzHGPGmMaWWMaYv9Xn8zxkwAFgDjnLt51TkDGGNSgEQR6eLcNBLYjBd/19gqoUEi0sD5t37knL36uz7Gqb7b74FbnL2HBgGHjlQhnRFjjNfegIuBeGAn8LSn43HROZ6LLRJuANY5bxdj68znA9ud9009HauLzn84MNv5uD2wEtgBfAMEeTo+F5xvXyDW+X1/BzTx9u8aeBHYCmwCPgGCvPG7Br7AtoMUY3/x33mq7xZbNfS289q2Edur6oyPrSOLlVLKx3lz1ZBSSqlK0ESglFI+ThOBUkr5OE0ESinl4zQRKKWUj9NEoLyaiDhEZN0xtwpH4orI3SJySw0cN0FEmp3B+8aIyAsi0kRE5lY3DqUqI+D0uyhVpx02xvSt7M7GmP+4MphKOA87WGoYsMTDsSgfoYlA+STn9BRfASOcm240xuwQkReAXGPMP0TkAeBu7FTfm40xN4hIU2AadkBTPjDJGLNBRMKxA4IisAOd5Jhj3YSdSrkedkLAe40xjhPiuR540vm5VwBRQLaInGOMudwV/wZKHaFVQ8rb1T+hauj6Y17LNsYMBP6NnavoRE8A/YwxvbEJAewo17XObU8BHzu3Pw8sNnYyuO+BNgAi0g24HhjqLJk4gAknHsgY8xV23qBNxphe2FG0/TQJKHfQEoHydhVVDX1xzP3r5by+AfhMRL7DTucAdkqPawCMMb+JSLiINMJW5Vzt3D5HRA469x8J9AdW2alyqM+pJ4XrhJ0yAKCBsetLKOVymgiULzOneHzEJdgL/OXAsyLSg4qn/y3vMwT4yBjzZEWBiEgs0AwIEJHNQHMRWQfcb4xZVPFpKFU9WjWkfNn1x9wvO/YFEfEDWhtjFmAXwGkMhAILcVbtiMhwIMPY9R+O3X4RdjI4sBOFjRORSOdrTUXkrBMDMcbEAHOw7QOvYidJ7KtJQLmDlgiUt6vv/GV9xE/GmCNdSINEZAX2B9H4E97nD3zqrPYR7Pq4Wc7G5A9FZAO2sfjIFMEvAl+IyBrgD+z0yRhjNovIM8AvzuRSDPwJ2FNOrGdjG5XvBV4r53WlXEJnH1U+ydlrKMYYk+HpWJTyNK0aUkopH6clAqWU8nFaIlBKKR+niUAppXycJgKllPJxmgiUUsrHaSJQSikfp4lAKaV83P8Dc46jQ7rvNIIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "# avgs=np.array([np.array(scores[max(i-100,0):i]).mean() for i in range(1,len(scores)+1)])\n",
    "plt.plot(np.arange(len(avgs)), avgs, c='r')\n",
    "plt.ylabel('scores')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load(f'actor_ckpt_1e61280.991e-31e-31e-302010sigma0.02theta0.04seed1.pth',map_location=device))\n",
    "agent.critic_local.load_state_dict(torch.load(f'critic_ckpt_1e61280.991e-31e-31e-302010sigma0.02theta0.04seed1.pth',map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 38.354999142698944 and counter 1001\n"
     ]
    }
   ],
   "source": [
    "# Watch it play\n",
    "episodes=1\n",
    "for i in range(episodes):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)  \n",
    "    counter=0# initialize the score (for each agent)\n",
    "    for _ in range(1002):  \n",
    "        actions = agent.act(states) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states\n",
    "        counter+=1# roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "print('Total score (averaged over agents) this episode: {} and counter {}'.format(np.mean(scores),counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9\tAverage Score: 0.07"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-70738951ff4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mavgs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mavgs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mddpg_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-70738951ff4a>\u001b[0m in \u001b[0;36mddpg_train\u001b[1;34m(agent, env, brain_name, n_episodes, max_t, target_score, eps, eps_decay)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m                   \u001b[1;31m# get the reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-a00bbc85b24a>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;31m#                 for _ in range(LEARN_NUM):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-a00bbc85b24a>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, experiences, gamma)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m# ----------------------- update target networks ----------------------- #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTAU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTAU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-a00bbc85b24a>\u001b[0m in \u001b[0;36msoft_update\u001b[1;34m(self, local_model, target_model, tau)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtarget_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_param\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m             \u001b[0mtarget_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlocal_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtarget_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mOUNoise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from collections import deque \n",
    "\n",
    "scores=[]# list containing scores from each episode\n",
    "avgs=[]\n",
    "\n",
    "def ddpg_train(agent,env,brain_name,n_episodes=400, max_t=2000,target_score=30,eps=EPSILON,eps_decay=EPSILON_DECAY):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode (for reacher environment, an episode is 1001 timesteps)\n",
    "        target_score: stopping point for training and saving checkpoint\n",
    "    Returns\n",
    "    ======\n",
    "        scores,avgs: The history of scores and the history of last 100 score averages\n",
    "        Also saves the checkpoint and the scores array (filepath `stats` given by agent (hyper)parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    stats=f'{testrun}'\n",
    "#     stats=f\"{agent.buffer_size}{agent.batch_size}{agent.gamma}{agent.tau}{agent.lr}{agent.update_every}\"\n",
    "#     stats+=f\"{agent.hidden_layer}{agent.hidden_layer_size}{agent.seed}{agent.network}{agent.time_aware}\"\n",
    "#     if agent.network==\"mhq_dfq\": stats+=f\"{agent.mh_size}{agent.hidden_layer_d}\"\n",
    "#     stats+=f\"{eps_start}{eps_end}{eps_decay}\"  #save_path a la tensorboard! \n",
    "    global scores# list containing scores from each episode\n",
    "    global avgs# list containing average of last 100 scores from each episode\n",
    "    scores=[]\n",
    "    avgs=[]\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state,eps)\n",
    "            env_info = env.step(action.astype(int))[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]  \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            eps=min(eps-eps_decay,0.001)\n",
    "            if done:\n",
    "                break       \n",
    "        \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            \n",
    "        if np.mean(scores_window)>=target_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.actor_local.state_dict(), f\"{stats+str(i_episode)}actor_local.ph\")\n",
    "            torch.save(agent.critic_local.state_dict(), f\"{stats+str(i_episode)}critic_local.ph\")\n",
    "            torch.save(agent.actor_target.state_dict(), f\"{stats+str(i_episode)}actor_target.ph\")\n",
    "            torch.save(agent.critic_target.state_dict(), f\"{stats+str(i_episode)}critic_target.ph\")\n",
    "            np.savetxt(f'{stats+str(i_episode)}scores',np.array(scores))\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            plt.plot(np.arange(len(scores)), scores)\n",
    "            avgs=np.array([np.array(scores[max(i-100,0):i]).mean() for i in range(1,len(scores)+1)])\n",
    "            plt.plot(np.arange(len(avgs)), avgs, c='r')\n",
    "            plt.ylabel('scores')\n",
    "            plt.xlabel('Episode #')\n",
    "            plt.show()\n",
    "            break\n",
    "    return scores,avgs\n",
    "scores,avgs=ddpg_train(agent,env,brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-815ff891826d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ad2ed9713636>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, add_noise)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;34m\"\"\"Returns actions for given state as per current policy.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected np.ndarray (got dict)"
     ]
    }
   ],
   "source": [
    "# agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "# agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "state = env.reset()\n",
    "for t in range(200):\n",
    "    action = agent.act(state, add_noise=False)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Explore\n",
    "\n",
    "In this exercise, we have provided a sample DDPG agent and demonstrated how to use it to solve an OpenAI Gym environment.  To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "- Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster than this benchmark implementation.  Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task!\n",
    "- Write your own DDPG implementation.  Use this code as reference only when needed -- try as much as you can to write your own algorithm from scratch.\n",
    "- You may also like to implement prioritized experience replay, to see if it speeds learning.  \n",
    "- The current implementation adds Ornsetein-Uhlenbeck noise to the action space.  However, it has [been shown](https://blog.openai.com/better-exploration-with-parameter-noise/) that adding noise to the parameters of the neural network policy can improve performance.  Make this change to the code, to verify it for yourself!\n",
    "- Write a blog post explaining the intuition behind the DDPG algorithm and demonstrating how to use it to solve an RL environment of your choosing.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
